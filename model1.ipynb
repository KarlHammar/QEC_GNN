{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "678381ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch_geometric.transforms\n",
    "import torch_geometric.data\n",
    "from torch_geometric.data import Data, ClusterData, DataLoader\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch.utils.data import random_split\n",
    "import random\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GraphConv, TopKPooling,GATConv,SAGPooling,LEConv,ASAPooling\n",
    "from torch_geometric.data import Batch\n",
    "from torch_scatter import scatter_add\n",
    "from torch.nn import Conv1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d64b369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "dataset = torch.load('data/MWPM_d7_bal/graphs_clean.pt')\n",
    "#dataset = torch.load('../data/stdc/dmixed_large/graphs_clean.pt')\n",
    "#dataset = torch.load('../data/stdc/dmixed(579)_p1/graphs_clean.pt')\n",
    "#dataset = torch.load('../data/stdc/d5p1/graphs_clean.pt')\n",
    "\n",
    "\n",
    "batchsize=64\n",
    "batch_train_splits = int(0.8*len(dataset)/batchsize)*batchsize\n",
    "#batch_val_splits = int(0.2*len(dataset)/batchsize)*batchsize\n",
    "batch_test_splits = int(((len(dataset)-batch_train_splits)/batchsize)*batchsize)\n",
    "#pint(batch_train_splits)\n",
    "#print(batch_test_splits)\n",
    "\n",
    "dataset_usage = len(dataset)-(batch_train_splits+batch_test_splits)\n",
    "\n",
    "if dataset_usage ==0:\n",
    "    train_dataset, test_dataset = random_split(dataset[:],[batch_train_splits,batch_test_splits],generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "else:\n",
    "    train_dataset, test_dataset = random_split(dataset[:-dataset_usage],[batch_train_splits,batch_test_splits],generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False)\n",
    "#val_loader = DataLoader(val_dataset, batch_size=batchsize, shuffle=False)\n",
    "\n",
    "print(len(test_loader))\n",
    "print(len(train_loader))\n",
    "#print(len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8606769",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConv_net(torch.nn.Module):\n",
    "    def __init__(self,hidden_channels,k,device):\n",
    "        super(GraphConv_net, self).__init__()\n",
    "        \n",
    "        self.conv = GraphConv(2,hidden_channels//2,aggr='add')\n",
    "        self.conv_1 = GraphConv(hidden_channels//2,hidden_channels,aggr='add')\n",
    "        \n",
    "        \n",
    "        #self.conv = GraphConv(2,4,aggr='add')\n",
    "        #self.conv = GATConv(2,hidden_channels,heads=6)\n",
    "        \n",
    "        #self.topkpoolX = TopKPooling(hidden_channels*6,ratio=int(k))\n",
    "        #self.topkpoolZ = TopKPooling(hidden_channels*6,ratio=int(k))\n",
    "        \n",
    "        self.topkpoolX = TopKPooling(hidden_channels,ratio=int(k))\n",
    "        self.topkpoolZ = TopKPooling(hidden_channels,ratio=int(k))\n",
    "        \n",
    "        #self.topkpoolX = ASAPooling(hidden_channels,ratio=k,GNN=GraphConv)\n",
    "        #self.topkpoolZ = ASAPooling(hidden_channels,ratio=k,GNN=GraphConv)\n",
    "        \n",
    "        \n",
    "        self.lin = torch.nn.Linear(hidden_channels*k,48)\n",
    "        self.lin2 = torch.nn.Linear(48,12)\n",
    "        \n",
    "        self.lin_f = torch.nn.Linear(12,1)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.lrelu = torch.nn.LeakyReLU(negative_slope=0.2)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.drop_out = torch.nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.k = k\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        \n",
    "        x = self.conv(x,edge_index, edge_weight=edge_weight)\n",
    "        x = self.tanh(x)\n",
    "        x = self.conv_1(x, edge_index, edge_weight=edge_weight)\n",
    "            \n",
    "        X, edge_index_1, edge_attr_1, batch_1 ,_,_ = self.topkpoolX(x,edge_index, edge_attr=edge_weight, batch=batch)\n",
    "        Z, edge_index_2, edge_attr_2 ,batch_2 ,_,_ = self.topkpoolZ(x,edge_index, edge_attr=edge_weight, batch=batch)\n",
    "\n",
    "\n",
    "        # Add empty nodes?\n",
    "        \n",
    "        if X.size(0)<(batch_1[-1]+1)*int(self.k):\n",
    "            #missing_dim = (batch_1[-1]+1)*self.k-X.size(0)\n",
    "\n",
    "            old_length_x = 0\n",
    "            for jx in range(batch_1[-1]+1):\n",
    "                if jx == batch[-1]:\n",
    "                    length_x = len(batch_1[batch_1==jx])\n",
    "                    first_part = X[0:length_x+old_length_x, :]\n",
    "                    new_row = torch.zeros(int(self.k)-length_x, self.hidden_channels,device=device)#*self.num_heads\n",
    "                    X = torch.cat([first_part, new_row],dim=0)\n",
    "                else:\n",
    "                    length_x = len(batch_1[batch_1==jx])\n",
    "                    first_half = X[0:length_x+old_length_x, :]\n",
    "                    second_half = X[length_x+old_length_x:, :]\n",
    "                    x_add = torch.zeros(int(self.k)-length_x, self.hidden_channels,device=device)#*self.num_heads\n",
    "                    X = torch.cat([first_half, x_add, second_half],dim=0)\n",
    "                    old_length_x += int(self.k)\n",
    "\n",
    "        if Z.size(0)<(batch_2[-1]+1)*int(self.k):\n",
    "\n",
    "            old_length_z = 0\n",
    "            for kx in range(batch_2[-1]+1):\n",
    "                #if Z.size(0) == (batch_2[-1]+1)*self.k:\n",
    "                #    continue\n",
    "                if kx == batch_2[-1]:\n",
    "                    length_z = len(batch_2[batch_2==kx])\n",
    "                    first_part = Z[0:length_z+old_length_z, :]\n",
    "                    new_row = torch.zeros(int(self.k)-length_z, self.hidden_channels,device=device)#self.num_heads\n",
    "                    Z = torch.cat([first_part, new_row],dim=0)\n",
    "                else:\n",
    "                    length_z = len(batch_2[batch_2==kx])\n",
    "                    first_half = Z[0:length_z+old_length_z, :]\n",
    "                    second_half = Z[length_z+old_length_z:, :]\n",
    "                    z_add = torch.zeros(int(self.k)-length_z, self.hidden_channels,device=device)#self.num_heads\n",
    "                    Z = torch.cat([first_half, z_add, second_half],dim=0)\n",
    "                    old_length_z += int(self.k)\n",
    "        \n",
    "        \n",
    "        X = torch.flatten(X)\n",
    "        Z = torch.flatten(Z)\n",
    "        #print(X.shape)\n",
    "        #print(Z.shape)\n",
    "\n",
    "        X = X.view(batch_1[-1]+1,self.hidden_channels*int(self.k))#self.num_heads*2)\n",
    "        Z = Z.view(batch_2[-1]+1,self.hidden_channels*int(self.k))#self.num_heads*2)\n",
    "        \n",
    "        \n",
    "        #print(X.shape)\n",
    "        #print(Z.shape)\n",
    "        \n",
    "        X = self.drop_out(X)\n",
    "        Z = self.drop_out(Z)\n",
    "        \n",
    "        \n",
    "        Z = self.lin(Z)\n",
    "        X = self.lin(X)\n",
    "        \n",
    "        \n",
    "        Z = self.tanh(Z)\n",
    "        X = self.tanh(X)\n",
    "        \n",
    "        Z = self.lin2(Z)\n",
    "        X = self.lin2(X)\n",
    "        \n",
    "        \n",
    "        Z = self.tanh(Z)\n",
    "        X = self.tanh(X)\n",
    "        \n",
    "        \n",
    "        \n",
    "        Z = self.lin_f(Z)\n",
    "        X = self.lin_f(X)\n",
    "        \n",
    "\n",
    "        return self.sigmoid(X),self.sigmoid(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df63160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device =torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aa32675",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3823162182651474\n",
      "Epoch: 005, Train Acc: 0.2994, Train Acc_X: 0.5568, Train Acc_Z: 0.5469, Test Acc: 0.2766\n",
      "1.3778904551955902\n",
      "Epoch: 010, Train Acc: 0.3039, Train Acc_X: 0.5622, Train Acc_Z: 0.5469, Test Acc: 0.2874\n",
      "1.3728782081347084\n",
      "Epoch: 015, Train Acc: 0.3128, Train Acc_X: 0.5660, Train Acc_Z: 0.5520, Test Acc: 0.2874\n",
      "1.3682353961862361\n",
      "Epoch: 020, Train Acc: 0.3147, Train Acc_X: 0.5711, Train Acc_Z: 0.5548, Test Acc: 0.2862\n",
      "1.3661606354840974\n",
      "Epoch: 025, Train Acc: 0.3233, Train Acc_X: 0.5737, Train Acc_Z: 0.5641, Test Acc: 0.2790\n",
      "1.3652680510402389\n",
      "Epoch: 030, Train Acc: 0.3342, Train Acc_X: 0.5743, Train Acc_Z: 0.5705, Test Acc: 0.2862\n",
      "1.3587064194243181\n",
      "Epoch: 035, Train Acc: 0.3355, Train Acc_X: 0.5730, Train Acc_Z: 0.5762, Test Acc: 0.3006\n",
      "1.3571441347241386\n",
      "Epoch: 040, Train Acc: 0.3473, Train Acc_X: 0.5883, Train Acc_Z: 0.5791, Test Acc: 0.3018\n",
      "1.3567090535684818\n",
      "Epoch: 045, Train Acc: 0.3524, Train Acc_X: 0.5899, Train Acc_Z: 0.5858, Test Acc: 0.3114\n",
      "1.3510608410566571\n",
      "Epoch: 050, Train Acc: 0.3536, Train Acc_X: 0.5896, Train Acc_Z: 0.5890, Test Acc: 0.2946\n",
      "1.3480531456835902\n",
      "Epoch: 055, Train Acc: 0.3495, Train Acc_X: 0.5915, Train Acc_Z: 0.5851, Test Acc: 0.3114\n",
      "1.3425490654519108\n",
      "Epoch: 060, Train Acc: 0.3552, Train Acc_X: 0.5938, Train Acc_Z: 0.5944, Test Acc: 0.3102\n",
      "1.3418792876100143\n",
      "Epoch: 065, Train Acc: 0.3610, Train Acc_X: 0.5922, Train Acc_Z: 0.5998, Test Acc: 0.2994\n",
      "1.3350011863544702\n",
      "Epoch: 070, Train Acc: 0.3654, Train Acc_X: 0.5931, Train Acc_Z: 0.5992, Test Acc: 0.3030\n",
      "1.3355541615696085\n",
      "Epoch: 075, Train Acc: 0.3747, Train Acc_X: 0.6017, Train Acc_Z: 0.6065, Test Acc: 0.2958\n",
      "1.3342026639708706\n",
      "Epoch: 080, Train Acc: 0.3760, Train Acc_X: 0.6043, Train Acc_Z: 0.6087, Test Acc: 0.3030\n",
      "1.32960031550045\n",
      "Epoch: 085, Train Acc: 0.3791, Train Acc_X: 0.6094, Train Acc_Z: 0.6129, Test Acc: 0.3150\n",
      "1.3271545122626698\n",
      "Epoch: 090, Train Acc: 0.3916, Train Acc_X: 0.6122, Train Acc_Z: 0.6224, Test Acc: 0.3042\n",
      "1.316794462593084\n",
      "Epoch: 095, Train Acc: 0.3769, Train Acc_X: 0.5995, Train Acc_Z: 0.6122, Test Acc: 0.2850\n",
      "1.3191535864291388\n",
      "Epoch: 100, Train Acc: 0.3916, Train Acc_X: 0.6202, Train Acc_Z: 0.6298, Test Acc: 0.3162\n",
      "1.316783730162971\n",
      "Epoch: 105, Train Acc: 0.4034, Train Acc_X: 0.6272, Train Acc_Z: 0.6352, Test Acc: 0.3078\n",
      "1.3126170973824942\n",
      "Epoch: 110, Train Acc: 0.3836, Train Acc_X: 0.6075, Train Acc_Z: 0.6237, Test Acc: 0.3138\n",
      "1.302884619209888\n",
      "Epoch: 115, Train Acc: 0.4107, Train Acc_X: 0.6228, Train Acc_Z: 0.6413, Test Acc: 0.2994\n",
      "1.2970452699480328\n",
      "Epoch: 120, Train Acc: 0.4053, Train Acc_X: 0.6186, Train Acc_Z: 0.6470, Test Acc: 0.3114\n",
      "1.3036869622218397\n",
      "Epoch: 125, Train Acc: 0.4187, Train Acc_X: 0.6301, Train Acc_Z: 0.6454, Test Acc: 0.3126\n",
      "1.2943971474459972\n",
      "Epoch: 130, Train Acc: 0.4212, Train Acc_X: 0.6419, Train Acc_Z: 0.6480, Test Acc: 0.3066\n",
      "1.278456708158668\n",
      "Epoch: 135, Train Acc: 0.4222, Train Acc_X: 0.6339, Train Acc_Z: 0.6547, Test Acc: 0.3102\n",
      "1.2708424280260573\n",
      "Epoch: 140, Train Acc: 0.4407, Train Acc_X: 0.6511, Train Acc_Z: 0.6623, Test Acc: 0.3126\n",
      "1.2625525099355932\n",
      "Epoch: 145, Train Acc: 0.4464, Train Acc_X: 0.6543, Train Acc_Z: 0.6661, Test Acc: 0.3305\n",
      "1.2426786252719724\n",
      "Epoch: 150, Train Acc: 0.4554, Train Acc_X: 0.6614, Train Acc_Z: 0.6690, Test Acc: 0.3281\n",
      "1.2165931017453475\n",
      "Epoch: 155, Train Acc: 0.4710, Train Acc_X: 0.6945, Train Acc_Z: 0.6693, Test Acc: 0.3737\n",
      "1.2006349718642544\n",
      "Epoch: 160, Train Acc: 0.4885, Train Acc_X: 0.7111, Train Acc_Z: 0.6719, Test Acc: 0.3808\n",
      "1.1755399276963339\n",
      "Epoch: 165, Train Acc: 0.4952, Train Acc_X: 0.7127, Train Acc_Z: 0.6811, Test Acc: 0.3928\n",
      "1.1735631822154597\n",
      "Epoch: 170, Train Acc: 0.4974, Train Acc_X: 0.7076, Train Acc_Z: 0.6827, Test Acc: 0.3964\n",
      "1.1606441608599245\n",
      "Epoch: 175, Train Acc: 0.5057, Train Acc_X: 0.7277, Train Acc_Z: 0.6834, Test Acc: 0.4036\n",
      "1.1661195481404432\n",
      "Epoch: 180, Train Acc: 0.4997, Train Acc_X: 0.7181, Train Acc_Z: 0.6843, Test Acc: 0.3988\n",
      "1.1278158747870317\n",
      "Epoch: 185, Train Acc: 0.5274, Train Acc_X: 0.7414, Train Acc_Z: 0.6996, Test Acc: 0.4180\n",
      "1.1152387004704503\n",
      "Epoch: 190, Train Acc: 0.5351, Train Acc_X: 0.7519, Train Acc_Z: 0.6993, Test Acc: 0.4156\n",
      "1.1099415850974512\n",
      "Epoch: 195, Train Acc: 0.5249, Train Acc_X: 0.7427, Train Acc_Z: 0.7025, Test Acc: 0.4395\n",
      "1.0891198612125605\n",
      "Epoch: 200, Train Acc: 0.5405, Train Acc_X: 0.7589, Train Acc_Z: 0.7018, Test Acc: 0.4455\n",
      "1.0965220725080975\n",
      "Epoch: 205, Train Acc: 0.5290, Train Acc_X: 0.7452, Train Acc_Z: 0.6987, Test Acc: 0.4084\n",
      "1.0691124366163438\n",
      "Epoch: 210, Train Acc: 0.5561, Train Acc_X: 0.7710, Train Acc_Z: 0.7207, Test Acc: 0.4419\n",
      "1.0435982912087776\n",
      "Epoch: 215, Train Acc: 0.5635, Train Acc_X: 0.7653, Train Acc_Z: 0.7264, Test Acc: 0.4395\n",
      "1.0449823618209524\n",
      "Epoch: 220, Train Acc: 0.5730, Train Acc_X: 0.7793, Train Acc_Z: 0.7248, Test Acc: 0.4647\n",
      "1.028677437881806\n",
      "Epoch: 225, Train Acc: 0.5845, Train Acc_X: 0.7899, Train Acc_Z: 0.7328, Test Acc: 0.4766\n",
      "1.0065008669678073\n",
      "Epoch: 230, Train Acc: 0.5909, Train Acc_X: 0.7915, Train Acc_Z: 0.7363, Test Acc: 0.4778\n",
      "0.9895391771942539\n",
      "Epoch: 235, Train Acc: 0.5944, Train Acc_X: 0.8087, Train Acc_Z: 0.7296, Test Acc: 0.5186\n",
      "0.9797002089400001\n",
      "Epoch: 240, Train Acc: 0.5976, Train Acc_X: 0.7860, Train Acc_Z: 0.7484, Test Acc: 0.5293\n",
      "0.9633240358511392\n",
      "Epoch: 245, Train Acc: 0.6295, Train Acc_X: 0.8326, Train Acc_Z: 0.7529, Test Acc: 0.5509\n",
      "0.9254304139171482\n",
      "Epoch: 250, Train Acc: 0.6228, Train Acc_X: 0.8208, Train Acc_Z: 0.7567, Test Acc: 0.5425\n",
      "0.8997882651490001\n",
      "Epoch: 255, Train Acc: 0.6403, Train Acc_X: 0.8473, Train Acc_Z: 0.7510, Test Acc: 0.5772\n",
      "0.8544892510716506\n",
      "Epoch: 260, Train Acc: 0.6693, Train Acc_X: 0.8610, Train Acc_Z: 0.7736, Test Acc: 0.6216\n",
      "0.8032935426824944\n",
      "Epoch: 265, Train Acc: 0.6846, Train Acc_X: 0.8709, Train Acc_Z: 0.7835, Test Acc: 0.6419\n",
      "0.7841676603264554\n",
      "Epoch: 270, Train Acc: 0.7111, Train Acc_X: 0.8820, Train Acc_Z: 0.8010, Test Acc: 0.6743\n",
      "0.7503174288883955\n",
      "Epoch: 275, Train Acc: 0.7063, Train Acc_X: 0.8654, Train Acc_Z: 0.8026, Test Acc: 0.6970\n",
      "0.7188732654066181\n",
      "Epoch: 280, Train Acc: 0.7315, Train Acc_X: 0.8871, Train Acc_Z: 0.8163, Test Acc: 0.7042\n",
      "0.6837938926596945\n",
      "Epoch: 285, Train Acc: 0.7433, Train Acc_X: 0.8906, Train Acc_Z: 0.8268, Test Acc: 0.7126\n",
      "0.6705839242072301\n",
      "Epoch: 290, Train Acc: 0.7519, Train Acc_X: 0.8941, Train Acc_Z: 0.8364, Test Acc: 0.7174\n",
      "0.6311751160956027\n",
      "Epoch: 295, Train Acc: 0.7643, Train Acc_X: 0.8992, Train Acc_Z: 0.8457, Test Acc: 0.7210\n",
      "0.6511792666523659\n",
      "Epoch: 300, Train Acc: 0.7653, Train Acc_X: 0.9011, Train Acc_Z: 0.8431, Test Acc: 0.7389\n",
      "0.6225208418190676\n",
      "Epoch: 305, Train Acc: 0.7736, Train Acc_X: 0.9027, Train Acc_Z: 0.8530, Test Acc: 0.7257\n",
      "0.6098054816374135\n",
      "Epoch: 310, Train Acc: 0.7653, Train Acc_X: 0.9024, Train Acc_Z: 0.8457, Test Acc: 0.7138\n",
      "0.5866319485424182\n",
      "Epoch: 315, Train Acc: 0.7771, Train Acc_X: 0.9088, Train Acc_Z: 0.8482, Test Acc: 0.7317\n",
      "0.5967477504211778\n",
      "Epoch: 320, Train Acc: 0.7876, Train Acc_X: 0.9088, Train Acc_Z: 0.8616, Test Acc: 0.7449\n",
      "0.5746793900339237\n",
      "Epoch: 325, Train Acc: 0.7892, Train Acc_X: 0.9139, Train Acc_Z: 0.8587, Test Acc: 0.7281\n",
      "0.5640126832471049\n",
      "Epoch: 330, Train Acc: 0.8026, Train Acc_X: 0.9155, Train Acc_Z: 0.8744, Test Acc: 0.7365\n",
      "0.5793317796528266\n",
      "Epoch: 335, Train Acc: 0.7774, Train Acc_X: 0.9056, Train Acc_Z: 0.8514, Test Acc: 0.7162\n",
      "0.5453299889488781\n",
      "Epoch: 340, Train Acc: 0.8071, Train Acc_X: 0.9136, Train Acc_Z: 0.8801, Test Acc: 0.7449\n",
      "0.5506022149709833\n",
      "Epoch: 345, Train Acc: 0.8103, Train Acc_X: 0.9225, Train Acc_Z: 0.8747, Test Acc: 0.7401\n",
      "0.5284010066827376\n",
      "Epoch: 350, Train Acc: 0.8198, Train Acc_X: 0.9251, Train Acc_Z: 0.8836, Test Acc: 0.7437\n",
      "0.5415573031487108\n",
      "Epoch: 355, Train Acc: 0.8029, Train Acc_X: 0.9114, Train Acc_Z: 0.8776, Test Acc: 0.7281\n",
      "0.5381859034387564\n",
      "Epoch: 360, Train Acc: 0.8275, Train Acc_X: 0.9302, Train Acc_Z: 0.8871, Test Acc: 0.7269\n",
      "0.525694594511175\n",
      "Epoch: 365, Train Acc: 0.8335, Train Acc_X: 0.9369, Train Acc_Z: 0.8871, Test Acc: 0.7425\n",
      "0.515682974642281\n",
      "Epoch: 370, Train Acc: 0.8304, Train Acc_X: 0.9298, Train Acc_Z: 0.8909, Test Acc: 0.7353\n",
      "0.5160983884794982\n",
      "Epoch: 375, Train Acc: 0.8208, Train Acc_X: 0.9279, Train Acc_Z: 0.8830, Test Acc: 0.7401\n",
      "0.494630920733572\n",
      "Epoch: 380, Train Acc: 0.8348, Train Acc_X: 0.9356, Train Acc_Z: 0.8897, Test Acc: 0.7293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4996895330216117\n",
      "Epoch: 385, Train Acc: 0.8517, Train Acc_X: 0.9372, Train Acc_Z: 0.9066, Test Acc: 0.7425\n",
      "0.4812610647014817\n",
      "Epoch: 390, Train Acc: 0.8533, Train Acc_X: 0.9401, Train Acc_Z: 0.9050, Test Acc: 0.7521\n",
      "0.49536427554846957\n",
      "Epoch: 395, Train Acc: 0.8339, Train Acc_X: 0.9340, Train Acc_Z: 0.8871, Test Acc: 0.7341\n",
      "0.4447791802567799\n",
      "Epoch: 400, Train Acc: 0.8654, Train Acc_X: 0.9404, Train Acc_Z: 0.9161, Test Acc: 0.7198\n",
      "0.4354955902598183\n",
      "Epoch: 405, Train Acc: 0.8667, Train Acc_X: 0.9506, Train Acc_Z: 0.9088, Test Acc: 0.7341\n",
      "0.4352524669817934\n",
      "Epoch: 410, Train Acc: 0.8721, Train Acc_X: 0.9480, Train Acc_Z: 0.9184, Test Acc: 0.7461\n",
      "0.4389888970514927\n",
      "Epoch: 415, Train Acc: 0.8760, Train Acc_X: 0.9528, Train Acc_Z: 0.9184, Test Acc: 0.7401\n",
      "0.4180203643215342\n",
      "Epoch: 420, Train Acc: 0.8836, Train Acc_X: 0.9557, Train Acc_Z: 0.9212, Test Acc: 0.7329\n",
      "0.42406157349309387\n",
      "Epoch: 425, Train Acc: 0.8830, Train Acc_X: 0.9538, Train Acc_Z: 0.9225, Test Acc: 0.7341\n",
      "0.4226649161711314\n",
      "Epoch: 430, Train Acc: 0.8801, Train Acc_X: 0.9554, Train Acc_Z: 0.9190, Test Acc: 0.7425\n",
      "0.4190470647280091\n",
      "Epoch: 435, Train Acc: 0.8852, Train Acc_X: 0.9557, Train Acc_Z: 0.9241, Test Acc: 0.7377\n",
      "0.40904015131982896\n",
      "Epoch: 440, Train Acc: 0.8833, Train Acc_X: 0.9560, Train Acc_Z: 0.9206, Test Acc: 0.7377\n",
      "0.40932550454052335\n",
      "Epoch: 445, Train Acc: 0.8855, Train Acc_X: 0.9573, Train Acc_Z: 0.9232, Test Acc: 0.7509\n",
      "0.40537973722890475\n",
      "Epoch: 450, Train Acc: 0.8865, Train Acc_X: 0.9538, Train Acc_Z: 0.9263, Test Acc: 0.7377\n",
      "0.4157565546757697\n",
      "Epoch: 455, Train Acc: 0.8817, Train Acc_X: 0.9547, Train Acc_Z: 0.9228, Test Acc: 0.7329\n",
      "0.4105763322562495\n",
      "Epoch: 460, Train Acc: 0.8814, Train Acc_X: 0.9531, Train Acc_Z: 0.9241, Test Acc: 0.7269\n",
      "0.4060377581689227\n",
      "Epoch: 465, Train Acc: 0.8913, Train Acc_X: 0.9544, Train Acc_Z: 0.9330, Test Acc: 0.7377\n",
      "0.41110137423465376\n",
      "Epoch: 470, Train Acc: 0.8893, Train Acc_X: 0.9566, Train Acc_Z: 0.9289, Test Acc: 0.7425\n",
      "0.40220079910615114\n",
      "Epoch: 475, Train Acc: 0.8906, Train Acc_X: 0.9592, Train Acc_Z: 0.9276, Test Acc: 0.7485\n",
      "0.40238377942483644\n",
      "Epoch: 480, Train Acc: 0.8893, Train Acc_X: 0.9576, Train Acc_Z: 0.9283, Test Acc: 0.7365\n",
      "0.4043564898165959\n",
      "Epoch: 485, Train Acc: 0.8881, Train Acc_X: 0.9585, Train Acc_Z: 0.9263, Test Acc: 0.7413\n",
      "0.3980292542130757\n",
      "Epoch: 490, Train Acc: 0.8887, Train Acc_X: 0.9585, Train Acc_Z: 0.9267, Test Acc: 0.7353\n",
      "0.39924253923909375\n",
      "Epoch: 495, Train Acc: 0.8941, Train Acc_X: 0.9611, Train Acc_Z: 0.9308, Test Acc: 0.7341\n"
     ]
    }
   ],
   "source": [
    "seed =4\n",
    "learning_rate = 0.0025#0.0025 #(0.005)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "model = GraphConv_net(16,10,device)\n",
    "model.to(device)\n",
    "model = model.double()\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min',factor=0.4,patience=10)\n",
    "\n",
    "\n",
    "#criterion = torch.nn.MSELoss().to(device)\n",
    "criterion = torch.nn.BCELoss().to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    acc = []\n",
    "    loss_holder = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        tar, idx = torch.max(data.y,dim=1)\n",
    "        int_tar = idx.cpu().numpy()\n",
    "        \n",
    "        #int_tar = np.array(idx)\n",
    "        #print(int_tar)\n",
    "        bin_tar = ((int_tar.reshape(-1,1) & (2**np.arange(2))) != 0).astype(int)\n",
    "        bin_target = bin_tar[:,::-1]\n",
    "        #print(bin_target)\n",
    "    \n",
    "        x_target = bin_target[:,0]\n",
    "        z_target = bin_target[:,1]\n",
    "\n",
    "        #rint(x_target)\n",
    "        #print(z_target)\n",
    "\n",
    "        \n",
    "        x_target = torch.from_numpy(x_target.copy()).double().to(device)\n",
    "        z_target = torch.from_numpy(z_target.copy()).double().to(device)\n",
    "\n",
    "    \n",
    "        out_X, out_Z = model(data)\n",
    "\n",
    "\n",
    "        loss_X = criterion(out_X, x_target.unsqueeze(-1))# Compute the loss\n",
    "        loss_Z = criterion(out_Z, z_target.unsqueeze(-1))\n",
    "        \n",
    "\n",
    "        loss = sum([loss_X, loss_Z])  # or loss = loss1 + loss2\n",
    "        loss_holder+=loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()# Update parameters based on gradients.\n",
    "    #acc.append([loss_X.item() ,loss_Z.item()])\n",
    "    loss_item = loss_holder/len(train_loader) \n",
    "    return loss_item\n",
    "    \n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct_Z = 0\n",
    "    correct_X = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        data = data.to(device)\n",
    "        \n",
    "        tar, idx = torch.max(data.y,dim=1)\n",
    "\n",
    "        #int_tar = np.array(idx)\n",
    "        int_tar = idx.cpu().numpy()\n",
    "        #print(int_tar)\n",
    "        bin_tar = ((int_tar.reshape(-1,1) & (2**np.arange(2))) != 0).astype(int)\n",
    "        bin_target = bin_tar[:,::-1]\n",
    "        #print(bin_target)\n",
    "        x_target = bin_target[:,0]\n",
    "        z_target = bin_target[:,1]\n",
    "    \n",
    "\n",
    "        x_target = torch.from_numpy(x_target.copy()).double().to(device)\n",
    "        z_target = torch.from_numpy(z_target.copy()).double().to(device)\n",
    "\n",
    "    \n",
    "\n",
    "        out_X, out_Z = model(data)\n",
    "        #print(out_X, out_Z)\n",
    "        target = torch.cat([x_target.unsqueeze(-1),z_target.unsqueeze(-1)],dim=1)\n",
    "        pred = torch.cat([out_X,out_Z],dim=1)\n",
    "        pred = torch.round(pred)      \n",
    "\n",
    "        correct_X += int((torch.round(out_X)== x_target.unsqueeze(-1)).sum())\n",
    "        correct_Z += int((torch.round(out_Z)== z_target.unsqueeze(-1)).sum())\n",
    "        \n",
    "        for ix in range(len(pred)):\n",
    "            corr=0\n",
    "            corr = int((pred[ix] == target[ix]).sum())  # Check against ground-truth labels.\n",
    "            if corr == 2:\n",
    "                correct =correct+ 1\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "    return correct/len(loader.dataset), correct_X / len(loader.dataset), correct_Z/len(loader.dataset) \n",
    "    \n",
    "\n",
    "\n",
    "train_list = []\n",
    "loss_list = []\n",
    "for epoch in range(1,500):\n",
    "    loss = train()\n",
    "    loss_list.append(loss)\n",
    "    scheduler.step(loss)\n",
    "    train_acc,train_acc_X,train_acc_Z = test(train_loader)\n",
    "    test_acc ,_ ,_ = test(test_loader) \n",
    "    if epoch % 5==0:\n",
    "        print(loss)\n",
    "        print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Train Acc_X: {train_acc_X:.4f}, Train Acc_Z: {train_acc_Z:.4f}, Test Acc: {test_acc:.4f}')\n",
    "    train_list.append([train_acc,test_acc,train_acc_X,train_acc_Z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b0d707",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"model1_d7_p05_4000_unbal.pt\"\n",
    "torch.save(model.state_dict(),PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff167d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf019c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0145b67da05c79438d9b05e8abf3e06706409424441d72602b06c6ec46656107"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
